{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.datasets import *\nimport cv2\nimport sklearn\nimport matplotlib.pyplot as plt\nimport os\ntf.compat.v1.disable_eager_execution()","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.compat.v1.reset_default_graph()","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\ntf.compat.v1.set_random_seed(42)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = \"../input/p800-data/224Frames/train\"\nval_path = \"../input/p800-data/224Frames/val\"\ntrain_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True,rotation_range= 45,horizontal_flip = True)\nval_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True,rotation_range= 45,horizontal_flip = True)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = train_data_generator.flow_from_directory(train_path,target_size = (256,256),color_mode = \"rgb\",class_mode = \"categorical\",batch_size = 100,shuffle = True)","execution_count":5,"outputs":[{"output_type":"stream","text":"Found 11707 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_gen = val_data_generator.flow_from_directory(val_path,target_size = (256,256),color_mode = \"rgb\",class_mode = \"categorical\",batch_size = 100,shuffle = True)","execution_count":6,"outputs":[{"output_type":"stream","text":"Found 2507 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"caps1_n_maps = 32\ncaps1_n_caps = caps1_n_maps*11*11\ncaps1_n_dims = 8\ncaps2_n_caps = 2\ncaps2_n_dims = 16","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conv1_p = {\n    \"strides\": 2,\n    \"filters\": 64,\n    \"padding\":\"VALID\",\n    \"activation\": \"relu\",\n    \"kernel_size\": 4\n}\nconv2_p = {\n    \"strides\": 2,\n    \"filters\": 128,\n    \"padding\":\"VALID\",\n    \"activation\": \"relu\",\n    \"kernel_size\": 6\n}\nconv3_p = {\n    \"strides\": 2,\n    \"filters\": 256,\n    \"padding\":\"VALID\",\n    \"activation\": \"relu\",\n    \"kernel_size\": 6\n}\nconv4_p = {\n    \"strides\": 2,\n    \"filters\": 256,\n    \"padding\":\"VALID\",\n    \"activation\": \"relu\",\n    \"kernel_size\": 8\n}\nconv1_params = {\n    \"strides\": 1,\n    \"filters\": 256,\n    \"padding\":\"VALID\",\n    \"activation\": \"relu\",\n    \"kernel_size\": 9\n}\nconv2_params = {\n    \"strides\": 2,\n    \"kernel_size\":9,\n    \"filters\": caps1_n_maps*caps1_n_dims,\n    \"padding\":\"VALID\",\n    \"activation\": \"relu\"\n}","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = tf.compat.v1.placeholder(shape = (None,256,256,3),dtype =tf.float32)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conv1 = tf.compat.v1.layers.conv2d(X,**conv1_p)\nconv2 = tf.compat.v1.layers.conv2d(conv1,**conv2_p)\nconv3 = tf.compat.v1.layers.conv2d(conv2,**conv3_p)\nconv4 = tf.compat.v1.layers.conv2d(conv3,**conv4_p)\nconv2_reshaped = tf.reshape(conv4,shape= (-1,caps1_n_caps,caps1_n_dims))\nprint(conv1)\nprint(conv2)\nprint(conv3)\nprint(conv4)\nprint(conv2_reshaped)","execution_count":10,"outputs":[{"output_type":"stream","text":"Tensor(\"conv2d/Relu:0\", shape=(None, 127, 127, 64), dtype=float32)\nTensor(\"conv2d_1/Relu:0\", shape=(None, 61, 61, 128), dtype=float32)\nTensor(\"conv2d_2/Relu:0\", shape=(None, 28, 28, 256), dtype=float32)\nTensor(\"conv2d_3/Relu:0\", shape=(None, 11, 11, 256), dtype=float32)\nTensor(\"Reshape:0\", shape=(None, 3872, 8), dtype=float32)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def squash(s,axis = -1,epsilon = 1e-9):\n    with tf.compat.v1.name_scope(\"Squashing\"):\n        square = tf.reduce_sum(tf.square(s),axis = axis,keepdims = True)\n        norm = tf.sqrt(square+epsilon)\n        squashed = square/(1.+square)\n        squashed = squashed/norm\n        return s*squashed","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"caps1_output= squash(conv2_reshaped)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"W_init = tf.compat.v1.random_normal(shape = (1,caps1_n_caps,caps2_n_caps,caps2_n_dims,caps1_n_dims))\nW = tf.Variable(W_init,name = \"WeightMatrix\")\nbatch_shape = tf.shape(X)[0]\nW_tiled = tf.tile(W,[batch_shape,1,1,1,1],name = \"W_tiled\")\n\nprint(W_tiled)\n\ncaps1_output_exp = tf.expand_dims(caps1_output,axis = -1)\ncaps1_output_exp = tf.expand_dims(caps1_output_exp,axis = 2)\ncaps1_output_ = tf.tile(caps1_output_exp,[1,1,caps2_n_caps,1,1])\n\nprint(caps1_output_)\n\ncaps_predicted = tf.matmul(W_tiled,caps1_output_)\n\nprint(caps_predicted)","execution_count":13,"outputs":[{"output_type":"stream","text":"Tensor(\"W_tiled:0\", shape=(None, 3872, 2, 16, 8), dtype=float32)\nTensor(\"Tile:0\", shape=(None, 3872, 2, 8, 1), dtype=float32)\nTensor(\"MatMul:0\", shape=(None, 3872, 2, 16, 1), dtype=float32)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"b_j = tf.zeros(shape =[tf.shape(X)[0],caps1_n_caps,caps2_n_caps,1,1],dtype = tf.float32)\nc_j = tf.nn.softmax(b_j,axis = 2)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_prediction = tf.multiply(c_j,caps_predicted)\nprint(weighted_prediction)\nweighted_sum = tf.reduce_sum(weighted_prediction,axis = 1,keepdims = True)\nprint(weighted_sum)\ncaps2_output_round_1 = squash(weighted_sum,axis = -2)\nprint(caps2_output_round_1)","execution_count":15,"outputs":[{"output_type":"stream","text":"Tensor(\"Mul:0\", shape=(None, 3872, 2, 16, 1), dtype=float32)\nTensor(\"Sum:0\", shape=(None, 1, 2, 16, 1), dtype=float32)\nTensor(\"Squashing_1/mul:0\", shape=(None, 1, 2, 16, 1), dtype=float32)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"caps2_output_round2 = tf.tile(caps2_output_round_1,[1,caps1_n_caps,1,1,1])\nprint(caps2_output_round2)\nprint(caps_predicted)\nagreement = tf.matmul(caps_predicted,caps2_output_round2,transpose_a = True)\nprint(agreement)","execution_count":16,"outputs":[{"output_type":"stream","text":"Tensor(\"Tile_1:0\", shape=(None, 3872, 2, 16, 1), dtype=float32)\nTensor(\"MatMul:0\", shape=(None, 3872, 2, 16, 1), dtype=float32)\nTensor(\"MatMul_1:0\", shape=(None, 3872, 2, 1, 1), dtype=float32)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_weight_round_2 = tf.add(agreement,b_j)\nsoftmax_round_2_b_j = tf.nn.softmax(raw_weight_round_2,axis = 2)\npredicted_output_round2 = tf.multiply(softmax_round_2_b_j,caps_predicted)\nprint(predicted_output_round2)\npredicted_sum_round2 = tf.reduce_sum(predicted_output_round2,axis = 1 ,keepdims = True)\nprint(predicted_sum_round2)\npredicted_squashed_round2 =  squash(predicted_sum_round2,axis = -2)\nprint(predicted_sum_round2)\ncaps2_output = predicted_squashed_round2","execution_count":17,"outputs":[{"output_type":"stream","text":"Tensor(\"Mul_1:0\", shape=(None, 3872, 2, 16, 1), dtype=float32)\nTensor(\"Sum_1:0\", shape=(None, 1, 2, 16, 1), dtype=float32)\nTensor(\"Sum_1:0\", shape=(None, 1, 2, 16, 1), dtype=float32)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def safe_norm(s, axis=-1, epsilon=1e-9, keep_dims=False, name=None):\n    with tf.name_scope(name):\n        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n                                     keepdims=keep_dims)\n        return tf.sqrt(squared_norm + epsilon)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\ny_pred","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"<tf.Tensor 'y_pred:0' shape=(None,) dtype=int64>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = tf.compat.v1.placeholder(shape=[None], dtype=tf.int64, name=\"y\")","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_plus = 0.9\nm_minus = 0.1\nlambda_ = 0.5\nT = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True,\n                              name=\"caps2_output_norm\")\npresent_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n                              name=\"present_error_raw\")\npresent_error = tf.reshape(present_error_raw, shape=(-1, 2),\n                           name=\"present_error\")\nabsent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n                             name=\"absent_error_raw\")\nabsent_error = tf.reshape(absent_error_raw, shape=(-1, 2),\n                          name=\"absent_error\")\nL = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n           name=\"L\")\n\nmargin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = 0.0005\nloss = margin_loss\ncorrect = tf.equal(y, y_pred, name=\"correct\")\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n\n\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.0001)\ntraining_op = optimizer.minimize(loss, name=\"training_op\")\n\ninit = tf.compat.v1.global_variables_initializer()\nsaver = tf.compat.v1.train.Saver()","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 100\nbatch_size = 100\nrestore_checkpoint = True\n\nn_iterations_per_epoch =100\nn_iterations_validation = 25\nbest_loss_val = np.infty\ncheckpoint_path = \"new\"\n\nwith tf.compat.v1.Session() as sess:\n    if restore_checkpoint and tf.compat.v1.train.checkpoint_exists(checkpoint_path):\n        saver.restore(sess, checkpoint_path)\n    else:\n        init.run()\n\n    for epoch in range(n_epochs):\n        for iteration in range(1, n_iterations_per_epoch):\n            # X_batch, y_batch = x_train[iteration*batch_size:(iteration+1)*batch_size],y_train_[iteration*batch_size:(iteration+1)*batch_size]\n            X_batch, y_batch = next(train_gen)\n            X_batch, y_batch = X_batch/255.0,np.argmax(y_batch,axis = 1)\n\n            _, loss_train = sess.run(\n                [training_op, loss],\n                feed_dict={X: X_batch.reshape([-1, 256, 256, 3]),\n                           y: y_batch})\n            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n                      iteration, n_iterations_per_epoch,\n                      iteration * 100 / n_iterations_per_epoch,\n                      loss_train),\n                  end=\"\")\n        loss_vals = []\n        acc_vals = []\n        for iteration in range(1, n_iterations_validation + 1):\n            # X_batch, y_batch = x_val[iteration*batch_size:(iteration+1)*batch_size],y_val_[iteration*batch_size:(iteration+1)*batch_size]\n            X_batch, y_batch = next(val_gen)\n            X_batch, y_batch = X_batch/255.0,np.argmax(y_batch,axis = 1)\n            loss_val, acc_val = sess.run(\n                    [loss, accuracy],\n                    feed_dict={X: X_batch.reshape([-1, 256, 256, 3]),\n                               y: y_batch})\n            loss_vals.append(loss_val)\n            acc_vals.append(acc_val)\n            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n                      iteration, n_iterations_validation,\n                      iteration * 100 / n_iterations_validation),\n                  end=\" \" * 10)\n        loss_val = np.mean(loss_vals)\n        acc_val = np.mean(acc_vals)\n        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n            epoch + 1, acc_val * 100, loss_val,\n            \" (improved)\" if loss_val < best_loss_val else \"\"))\n\n        # And save the model if it improved:\n        if loss_val < best_loss_val:\n            save_path = saver.save(sess, checkpoint_path)\n            best_loss_val = loss_val","execution_count":26,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n  warnings.warn('This ImageDataGenerator specifies '\n/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n  warnings.warn('This ImageDataGenerator specifies '\n","name":"stderr"},{"output_type":"stream","text":"Epoch: 1  Val accuracy: 77.3200%  Loss: 0.153680 (improved)\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"'latest_filename' collides with 'save_path': 'checkpoint' and 'checkpoint'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-99a2e4dde787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# And save the model if it improved:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mbest_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         raise ValueError(\n\u001b[1;32m   1168\u001b[0m             \u001b[0;34m\"'latest_filename' collides with 'save_path': '%s' and '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             (latest_filename, save_path))\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     if (not context.executing_eagerly() and\n","\u001b[0;31mValueError\u001b[0m: 'latest_filename' collides with 'save_path': 'checkpoint' and 'checkpoint'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}